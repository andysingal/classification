{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPgrazIxEQAg+QIVHTorSvL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andysingal/classification/blob/main/pytorch_class.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XX8cokR2Vs3A"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install pycrayon"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "import sys\n",
        "import random\n",
        "import time\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pylab as pl\n",
        "\n",
        "import torch\n",
        "\n",
        "%matplotlib inline\n",
        "from matplotlib_inline import backend_inline\n",
        "from matplotlib.colors import ListedColormap\n",
        "from distutils.version import LooseVersion\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from pycrayon import (\n",
        "    CrayonClient,\n",
        ")\n",
        "\n",
        "from IPython import (\n",
        "    display,\n",
        ")\n",
        "\n",
        "from IPython.display import (\n",
        "    Image,\n",
        "    clear_output,\n",
        ")\n",
        "\n",
        "sys.path.insert(0, '..')\n",
        "# get matplotlib configuration\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "8MlWhlh7Vt32"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_t = torch.randn(3, 5, 5)\n",
        "weights = torch.tensor([0.2126, 0.7152, 0.0722])\n",
        "batch_t = torch.randn(2, 3, 5, 5)\n",
        "\n",
        "img_gray_naive = img_t.mean(-3)\n",
        "batch_gray_naive = batch_t.mean(-3)\n",
        "img_gray_naive.shape, batch_gray_naive.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-bYFsdEIv7F",
        "outputId": "7113f1e9-7da5-46c5-c5e5-d8d2ffb2279f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([5, 5]), torch.Size([2, 5, 5]))"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unsqueezed_weights = weights.unsqueeze(-1).unsqueeze_(-1)\n",
        "img_weights = (img_t * unsqueezed_weights)\n",
        "batch_weights = (batch_t * unsqueezed_weights)\n",
        "img_gray_weighted = img_weights.sum(-3)\n",
        "batch_gray_weighted = batch_weights.sum(-3)\n",
        "batch_weights.shape, batch_t.shape, unsqueezed_weights.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_EDxNzRJdRB",
        "outputId": "b584be56-098c-4396-edd8-fc5e0f1b3353"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([2, 3, 5, 5]), torch.Size([2, 3, 5, 5]), torch.Size([3, 1, 1]))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#A dtype for every occasion"
      ],
      "metadata": {
        "id": "W02d8b0pKVTT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- As we will see in future chapters, computations happening in neural networks are typically executed with 32-bit floating-point precision. \n",
        "\n",
        "- Higher precision, like 64-bit, will not buy improvements in the accuracy of a model and will require more memory and computing time. \n",
        "\n",
        "- The 16-bit floating-point, half-precision data type is not present natively in standard CPUs, but it is offered on modern GPUs. It is possible to switch to half-precision to decrease the footprint of a neural network model if needed, with a minor impact on accuracy."
      ],
      "metadata": {
        "id": "WZjNQiT4KkuM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tensors can be used as indexes in other tensors. In this case, PyTorch expects indexing tensors to have a 64-bit integer data type. Creating a tensor with integers as arguments, such as using torch.tensor([2, 2]), will create a 64-bit integer tensor by default. As such, we’ll spend most of our time dealing with float32 and int64."
      ],
      "metadata": {
        "id": "qPy6tVbMlKt_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "double_points = torch.ones(10, 2, dtype=torch.double)\n",
        "short_points = torch.tensor([[1, 2], [3, 4]], dtype=torch.short)"
      ],
      "metadata": {
        "id": "_W8skKb-KXHb"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "point_54 = torch.randn(5,dtype=torch.double)\n",
        "point_1 = point_54.to(torch.short)\n",
        "point_1 * point_54\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HoBonsNql92E",
        "outputId": "094945e9-a724-41ae-faee-cf806db404a5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 0., 0., 0., 0.], dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#The tensor API"
      ],
      "metadata": {
        "id": "kP-aKHLumw9T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.ones(3, 2)\n",
        "a_t = torch.transpose(a, 0, 1)\n",
        "a.shape, a_t.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n98VDad7nAL8",
        "outputId": "04e3b574-f61f-448c-bb07-05cfbd7ac563"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([3, 2]), torch.Size([2, 3]))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tensors: Scenic views of storage"
      ],
      "metadata": {
        "id": "SXPAZ3W0oDFA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
        "points.storage()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j071joZwoFfw",
        "outputId": "4218309b-63fe-4368-e516-531a4b613f33"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-8575ff47c9e0>:2: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  points.storage()\n",
            "/usr/local/lib/python3.9/dist-packages/IPython/lib/pretty.py:700: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  output = repr(obj)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/storage.py:645: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  return str(self)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/storage.py:636: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  f'device={self.device}) of size {len(self)}]')\n",
            "/usr/local/lib/python3.9/dist-packages/torch/storage.py:637: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  if self.device.type == 'meta':\n",
            "/usr/local/lib/python3.9/dist-packages/torch/storage.py:640: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  data_str = ' ' + '\\n '.join(str(self[i]) for i in range(self.size()))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              " 4.0\n",
              " 1.0\n",
              " 5.0\n",
              " 3.0\n",
              " 2.0\n",
              " 1.0\n",
              "[torch.storage.TypedStorage(dtype=torch.float32, device=cpu) of size 6]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even though the tensor reports itself as having three rows and two columns, the storage under the hood is a contiguous array of size 6. In this sense, the tensor just knows how to translate a pair of indices into a location in the storage."
      ],
      "metadata": {
        "id": "62gsdM7Mpf3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
        "points_storage = points.storage()\n",
        "points_storage[0] = 2.0\n",
        "points"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_5cpTQepiVq",
        "outputId": "c26a2590-aed2-4e7b-be33-ea7e39e762d3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-91ddb5d95c0a>:2: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  points_storage = points.storage()\n",
            "<ipython-input-13-91ddb5d95c0a>:3: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  points_storage[0] = 2.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[2., 1.],\n",
              "        [5., 3.],\n",
              "        [2., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tensor metadata: Size, offset, and stride\n",
        "In order to index into a storage, tensors rely on a few pieces of information that, together with their storage, unequivocally define them: size, offset, and stride.\n",
        "\n",
        "1.  The size (or shape, in NumPy parlance) is a tuple indicating how many elements across each dimension the tensor represents.\n",
        "2.   The storage offset is the index in the storage corresponding to the first element in the tensor.\n",
        "3. The stride is the number of elements in the storage that need to be skipped over to obtain the next element along each dimension.\n"
      ],
      "metadata": {
        "id": "N_WSEluEqbcc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
        "second_point = points[1]\n",
        "second_point.storage_offset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMF2I8BwpiYk",
        "outputId": "e8dbedf8-1c2d-4612-aa19-cf85c9f3eb3c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "second_point.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgmdTsampicU",
        "outputId": "f5f8869e-a1cc-43af-c951-40bb77a07019"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "second_point.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3Ypolaopifj",
        "outputId": "61a6ecec-e9d8-4581-a55d-107d259f4763"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The stride is a tuple indicating the number of elements in the storage that have to be skipped when the index is increased by 1 in each dimension. For instance, our points tensor has a stride of (2, 1):\n",
        "\n"
      ],
      "metadata": {
        "id": "LGdpSDQAs_RT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "points.stride()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgPKxp-Rpiij",
        "outputId": "0fc025ed-9bec-4769-c72f-3dacbb784e26"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
        "second_point = points[1]\n",
        "second_point[0] = 10.0\n",
        "points"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hb4GwAJdpilt",
        "outputId": "b89dbada-5328-4445-9c50-b664abd28268"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 4.,  1.],\n",
              "        [10.,  3.],\n",
              "        [ 2.,  1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
        "points_t = points.t()\n",
        "points_t"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nz1ZiM53piok",
        "outputId": "95c7d0bf-0159-4536-d069-6fefb364faab"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[4., 5., 2.],\n",
              "        [1., 3., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Transposing in higher dimensions\n",
        "\n",
        "Transposing in PyTorch is not limited to matrices. We can transpose a multidimensional array by specifying the two dimensions along which transposing (flipping shape and stride) should occur:"
      ],
      "metadata": {
        "id": "c_PtapSculSZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "some_t = torch.ones(3, 4, 5)\n",
        "transpose_t = some_t.transpose(0, 2)\n",
        "some_t.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOBdLIy1pisN",
        "outputId": "ba1acbdb-1a46-44ce-8313-76d2dabc3087"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 4, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Contiguous tensors\n",
        "\n",
        "Some tensor operations in PyTorch only work on contiguous tensors, such as view, which we’ll encounter in the next chapter. In that case, PyTorch will throw an informative exception and require us to call contiguous explicitly. It’s worth noting that calling contiguous will do nothing (and will not hurt performance) if the tensor is already contiguous."
      ],
      "metadata": {
        "id": "5GXeINpHvTvU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "points.is_contiguous()\n",
        " "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpKzt8ILpivr",
        "outputId": "09bd465a-eef5-437a-fb49-f50e40ab197e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Moving tensors to the GPU"
      ],
      "metadata": {
        "id": "llBzEg-Xvvse"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EFTWKAdgvuh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QALK1Xk0vulF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZiiV5C8pvuob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nHfQ96Snvurf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ak5-xxazvuut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KjjWVGfDvuyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GaOgaVq2vu1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kaa-_ScGvu4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YbdFbYQGvu7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z3PLphWqvu_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GHzUfqzxvvCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fwlEcQ80vvFe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}